<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-generate-toc again -->
**Table of Contents**
* [简介](#简介)
* [分类](#分类)
	* [1 Logistic Regression](#1-Logistic Regression)
	* [2 Softmax Regression](#2-Softmax Regression)
	* [3 Factoriztion Machine](#3-Factoriztion Machine)
	* [4 SVM](#4-SVM)
	* [5 随机森林](#5-随机森林)
	* [6 BP神经网络](#6-BP神经网络)
* [回归](#回归)
	* [1 线性回归](#1-线性回归)
	* [2 岭回归和Lasso回归](#2-岭回归和Lasso回归)
	* [3 CART树回归](#3-CART树回归)
* [聚类](#聚类)
	* [1 K-Means](#1-K-Means)
	* [2 Mean Shift](#2-Mean Shift)
	* [3 DBSCAN](#3-DBSCAN)
	* [4 Label Propagation](#4-Label Propagation)
* [推荐](#推荐)
	* [1 协同过滤](#1-协同过滤)
	* [2 基于矩阵分解的算法](#2-基于矩阵分解的算法)
	* [3 基于图的推荐算法](#3-基于图的推荐算法)
* [推荐算法](#推荐算法)
	* [1 AutoEncoder](#1-AutoEncoder)
	* [2 卷积神经网络](#2-卷积神经网络)

<!-- markdown-toc end -->
# 简介
基于书《Python机器学习算法》整理


# 分类
根据样本特征划分到制定类/监督学习算法
通过分类算法得到样本特征到样本标签之间的映射关系
## 1 Logistic Regression
0. 逻辑回归通过训练正负样本得到假设函数，复杂度低，易实现
1. 找到超平面Wx+b=0
阈值函数:Sigmoid
概率统计:最大似然估计
2. 梯度下降法
针对模型构建损失函数l，优化找到最优参数W
选择初始点->方向/步长更新->满足终止条件
凸优化:局部最优==>全局最优
最小二乘、岭回归、Logistic回归是凸优化问题
3. 主函数
导入训练数据->训练LR模型->保存最终模型
## 2 Softmax Regression
0. Softmax回归:相比Logistic Regression,能解决多分类问题
1. 模型
[公式]
2. 梯度下降法更新求解
3. Softmax中存在多组最优解;Logistic是特殊化的Softmax
## 3 Factorization Machine
0. 因子分解机:对Logistic Regression的拓展;包含逻辑回归的线性项和非线性的交叉项
1. 模型:
度:考虑互异特征向量的相互关系
交叉项系数:矩阵分解
2. 算法求解:基于随机梯度
## 4 SVM
1. 概念
感知机:直接使用“通误分类”的样本到超平面距离作为损失函数;初始值和样本顺序都由影响
函数间隔:分类的正确性和确信度
集合间隔:样本到超平面的距离
支持向量机:间隔最小化
支持向量:与分割超平面距离最近的样本;确定最终分隔超平面时只有支持向量起作用
线性支持向量机:引入松弛变量
2. 训练
对偶算法:待约束的最小优化问题求解
核函数：非线性==>线性
序列最小最优化问题SMOL将大问题划分为一系列小的问题，对子问题求解达到对偶问题的求解过程
## 5 随机森林
集成学习:多个分类模型预测结果组合，主要分为bagging和boosting
0. 随机森林对数据集采样生成多个不同的数据集，每个数据集训练一棵分类数，结合预测结果
1. 概念
决策树: 某一维属性的值，划分到不同类别
最佳划分标准：信息增益，增益率，基尼指数
熵: 信息熵
信息增益:信息熵的减少两
增益率:
基尼指数:
2. CART树构建:
(1)遍历且分店，寻找最佳切分属性和最佳切分点，划分子集
(2)重复直至满足:每一叶节点找到最佳划分属性及最佳切分点
(3)生成CART决策树
3. 集成学习:训练多个分类器解决同一个问题
Bagging:训练样本有放回的抽取
Boosting:顺序第给训练集中数据项重新加权
4. 随机森林:自助法重采样，生成多个分类树组成随机森林;每棵树具有相同分布，分类误差取决于相关性
决策树只分裂，不剪枝
输入特征个数k可以取log2(n)
## 6 BP神经网络
0. 三层神经网络结构的浅层神经网络
1. 基本单位:神经元
激活函数:Sigmoid,tanh
参数:层数，偏置和权重
2. 
神经元输出计算:前向传播
损失函数:正则项防止过拟合;学习率
反向传播算法:计算神经元输出值，输出计算完成后计算残差
3. 学习过程
(1)初始化:权重、偏置、网络层结构、激活函数
(2)循环计算
(3)正向循环，计算误差
(4)反向传播，调整参数
(5)返回最终模型


# 回归
监督学习算法
和分类的区别是标签是连续值
## 1 线性回归
0. 求解目标:平方误差的最小值
1. 概念
最小二乘
广义逆
牛顿法:一阶导数和二阶导数进行迭代;基本牛顿法/全局牛顿法
## 2 岭回归和Lasso回归
都是正则化的特征选择方法
0. 解决最小二乘无法解决的强线性相关性问题
1. 岭回归:平方误差基础上增加L2正则项
2. Lasso回归:平方误差基础上增加L1正则项
3. BFGS校正算法
L-BFGS:只保存最近的m次迭代，降低存储空间
## 3 CART树回归
0. 局部回归模型;数据集切成多份，每一份单独建模，参数确定后无需改变
1. 选择样本与平均值的差的平方和作为划分指标
2. 剪枝:防止过拟合
前剪枝:对深度进行控制
后剪枝:过拟合则合并叶子结点


# 聚类
无监督学习算法
相似属性聚集到一个类中
## 1 K-means
0. 相似个体划分统一类别
闵可夫斯基距离(p)
曼哈顿距离(p=1)
欧式距离(p=2)
1. 步骤
初始化聚类中心数k->{计算样本与聚类中心相似度并划分/更新聚类中心}->输出
2. K-means++
先选一个点，然后概率选择则距离最大，最终完成聚类中心初始化
## 2 Mean Shift
相比K-means不需事先指定类别个数
0. 通过给定区域样本均值确定聚类中心，不断更新
1. 概念
Mean Shift向量:漂移均值向量求和再求平均
核函数:作用是使得随着样本与被漂移点的距离不同，漂移点对均值漂移向量贡献也不同;常用高斯核函数
N:输入空间    H:特征空间       h:带宽
2. 算法:
(1)计算漂移均值
(2)移动到漂移处
(3)更新直到满足停止条件
3. 解释:Mean Shift向量总是指向概率密度增加的方向
## 3 DBSCAN 
用于非球状簇
0. 基于密度的聚类算法[Density-Based Spatial Clustering of Application with Noise]
1. 概念
核心点:点的邻域内至少包含MinPts(最小样本数)的样本
边界点:样本数少于MinPts
噪音点:既不是核心点，也不是边界点
直接密度可达:核心点邻域内
密度可达:序列可达
密度连接:两个点存在一个公共的密度可达点
2. 算法原理:导出最大的密度连接样本的集合
(1)给定邻域参数和MinPts，确定所有的核心对象
(2)对每一个核心对象进行(3)
(3)选择任一个未处理的核心对象，找到密度可达的样本生成簇
(4)重复(2)和(3)
## 4 Label Propagation 
0. 社区:分组
划分方法:凝聚[不断合并不同社区]/分裂[不断删除网络中边]/直接近似求解模块度函数
评价标准:模块度
1. 基于标签传播的局部社区划分方法
2. 标签传播:
同步更新
异步更新
停止di(Cm)>di(Cj)
3. 过程
(初始化唯一标签->设置代数t->标签传播->)


# 推荐
推荐算法描述:
f:C×S->R
C:用户集  S:商品集
## 1 协同过滤
0. 概述:历史行为数据挖掘偏好，基于偏好对用户进行群组划分，推荐品味相似项。计算推荐结果只与用户对项的评分有关。大数据量下实时推荐较差
顺序:先从用户端找项(相似度计算)，用项分组，在分组里找值得推荐产品
1. 相似度度量方法：
(1)  欧式距离
(2)  皮尔逊相关系数: 量级不敏感
(3)  余弦相似度
2. 协同过滤
(1)  基于用户的：用户相似度矩阵->为未打分项打分
(2)  基于商品的：商品相似度矩阵->为未打分项打分
3. 推荐主函数
导入用户商品数据->计算相似性->相似性推荐->top_K推荐
## 2 基于矩阵分解的算法
0. 概述：对用户-商品矩阵分解，利用分解后的矩阵预测原始矩阵的未打分项
1. 矩阵分解后会产生损失;损失函数可以用梯度下降法求解，可以更新变量直至收敛;加入正则项可以有较好的泛化能力。
2. 通过矩阵乘法进行预测
3. 主函数:
导入用户商品矩阵->梯度下降法进行分解->用分解的矩阵数据进行预测->top_K推荐
## 3 基于图的推荐算法
0. 概述：计算相对结点重要性从而进行推荐
1. 二部图
2. PageRank算法
(1)两个假设：数量假设/质量假设
(2)过程: 有向图转邻接矩阵(行代表出链)->链接概率矩阵(归一化)->概率转移矩阵(转置)->修改转移矩阵(加入跳出概率)->迭代求解
3. PersonalRank算法
PageRank的变形，用来计算商品结点Dj相对于用户结点U的重要性。
过程：初始化->游走
4. 主函数:
导入用户商品矩阵->转换成邻接表->PersonalRank计算->根据rank结果进行商品推荐


# 深度网络
前面的内容可以说是特征处理，需要大量鲜艳知识
深度学习自动提取特征，这一过程称为特征学习
## 1 AutoEncoder
0. 最基本的特征学习方式，通过重构输入数据达到自我学习目的
1. 结构： ==>输入层--编码->隐含层--解码->输出层==> 
使用均方误差或交叉熵作为重构误差
2. 降噪自解码器：输入数据加入噪音，使学习的编码器具有更强鲁棒性
3. 构建网络
(1) 无监督逐层训练: 依次训练多个降噪自编码器
(2) 有监督微调:将训练好的多个降噪自编码器的编码Encoder层组合起来
## 2 卷积神经网络
0. 充分利用图像数据局部相关性，减少参数个数
1. 重要概念
(1) 稀疏连接:对数据局部区域进行建模，发现局部特性
(2) 共享权值:减少优化参数
子采样:解决图像中的平移不变性
感受野:被选择的子集大小;一组感受野中参数相互共享
(3) 池化:一般采用最大池化,降低计算量
2.  基础模型: 卷积层、池化层、全连接层
(1) 卷积操作
(2) max-pooling
(3) MLP:卷积层和下采样层交替叠加




